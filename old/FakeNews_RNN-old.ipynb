{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build an RNN to identify unreliable news articles\n",
    "\n",
    "This notebook applies an RNN to identify when an article might be fake news. The data were obtained from the [Fake News Competition on Kaggle](https://www.kaggle.com/c/fake-news). \n",
    "\n",
    ">Using an RNN rather than a strictly feedforward network is more accurate since we can include information about the *sequence* of words. \n",
    "\n",
    "**Performance on real test set from Kaggle:** My Kaggle submission resulted in **92.09%** for private score and  **91.41%** for public score.\n",
    "\n",
    "**Credit:** This is a side project for [PyTorch Scholarship Challenge from Facebook](https://www.udacity.com/facebook-pytorch-scholarship), which uses the [Sentiment_RNN](https://github.com/udacity/deep-learning-v2-pytorch/blob/master/sentiment-rnn/Sentiment_RNN_Solution.ipynb) template from the program. \n",
    "\n",
    "### Network Architecture\n",
    "\n",
    "The architecture for this network is shown below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/minhkhang1795/FakeNews_RNN/master/assets/network_diagram.png\" width=40%>\n",
    "\n",
    ">**First, we'll pass in words to an embedding layer.** We need an embedding layer because we have tens of thousands of words, so we'll need a more efficient representation for our input data than one-hot encoded vectors. You should have seen this before from the Word2Vec lesson. You can actually train an embedding with the Skip-gram Word2Vec model and use those embeddings as input, here. However, it's good enough to just have an embedding layer and let the network learn a different embedding table on its own. *In this case, the embedding layer is for dimensionality reduction, rather than for learning semantic representations.*\n",
    "\n",
    ">**After input words are passed to an embedding layer, the new embeddings will be passed to LSTM cells.** The LSTM cells will add *recurrent* connections to the network and give us the ability to include information about the *sequence* of words in the article data. \n",
    "\n",
    ">**Finally, the LSTM outputs will go to a sigmoid output layer.** We're using a sigmoid function because positive (or fake news) = 1 and negative = 0, and a sigmoid will output predicted, sentiment values between 0-1. \n",
    "\n",
    "We don't care about the sigmoid outputs except for the **very last one**; we can ignore the rest. We'll calculate the loss by comparing the output at the last time step and the training label (pos or neg).\n",
    "\n",
    "## Load and visualize the data\n",
    "**train.csv**: A full training dataset with the following attributes:\n",
    "\n",
    "- **id**: unique id for a news article\n",
    "- **title**: the title of a news article\n",
    "- **author**: author of the news article\n",
    "- **text**: the text of the article; could be incomplete\n",
    "- **label**: a label that marks the article as potentially unreliable\n",
    "    - 1: unreliable\n",
    "    - 0: reliable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Jackie Mason: Hollywood Would Love Trump if He...</td>\n",
       "      <td>Daniel Nussbaum</td>\n",
       "      <td>In these trying times, Jackie Mason is the Voi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Life: Life Of Luxury: Elton John’s 6 Favorite ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>Ever wonder how Britain’s most iconic pop pian...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Benoît Hamon Wins French Socialist Party’s Pre...</td>\n",
       "      <td>Alissa J. Rubin</td>\n",
       "      <td>PARIS  —   France chose an idealistic, traditi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Excerpts From a Draft Script for Donald Trump’...</td>\n",
       "      <td>nan</td>\n",
       "      <td>Donald J. Trump is scheduled to make a highly ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>A Back-Channel Plan for Ukraine and Russia, Co...</td>\n",
       "      <td>Megan Twohey and Scott Shane</td>\n",
       "      <td>A week before Michael T. Flynn resigned as nat...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Obama’s Organizing for Action Partners with So...</td>\n",
       "      <td>Aaron Klein</td>\n",
       "      <td>Organizing for Action, the activist group that...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>BBC Comedy Sketch \"Real Housewives of ISIS\" Ca...</td>\n",
       "      <td>Chris Tomlinson</td>\n",
       "      <td>The BBC produced spoof on the “Real Housewives...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Russian Researchers Discover Secret Nazi Milit...</td>\n",
       "      <td>Amando Flavio</td>\n",
       "      <td>The mystery surrounding The Third Reich and Na...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>US Officials See No Link Between Trump and Russia</td>\n",
       "      <td>Jason Ditz</td>\n",
       "      <td>Clinton Campaign Demands FBI Affirm Trump's Ru...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Re: Yes, There Are Paid Government Trolls On S...</td>\n",
       "      <td>AnotherAnnie</td>\n",
       "      <td>Yes, There Are Paid Government Trolls On Socia...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>In Major League Soccer, Argentines Find a Home...</td>\n",
       "      <td>Jack Williams</td>\n",
       "      <td>Guillermo Barros Schelotto was not the first A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Wells Fargo Chief Abruptly Steps Down - The Ne...</td>\n",
       "      <td>Michael Corkery and Stacy Cowley</td>\n",
       "      <td>The scandal engulfing Wells Fargo toppled its ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>Anonymous Donor Pays $2.5 Million To Release E...</td>\n",
       "      <td>Starkman</td>\n",
       "      <td>A Caddo Nation tribal leader has just been fre...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>FBI Closes In On Hillary!</td>\n",
       "      <td>The Doc</td>\n",
       "      <td>FBI Closes In On Hillary! Posted on Home » Hea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>Chuck Todd: ’BuzzFeed Did Donald Trump a Polit...</td>\n",
       "      <td>Jeff Poor</td>\n",
       "      <td>Wednesday after   Donald Trump’s press confere...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>News: Hope For The GOP: A Nude Paul Ryan Has J...</td>\n",
       "      <td>nan</td>\n",
       "      <td>Email \\nSince Donald Trump entered the electio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>Monica Lewinsky, Clinton Sex Scandal Set for ’...</td>\n",
       "      <td>Jerome Hudson</td>\n",
       "      <td>Screenwriter Ryan Murphy, who has produced the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>Rob Reiner: Trump Is ’Mentally Unstable’ - Bre...</td>\n",
       "      <td>Pam Key</td>\n",
       "      <td>Sunday on MSNBC’s “AM Joy,” actor and director...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>Massachusetts Cop’s Wife Busted for Pinning Fa...</td>\n",
       "      <td>nan</td>\n",
       "      <td>Massachusetts Cop’s Wife Busted for Pinning Fa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Abortion Pill Orders Rise in 7 Latin American ...</td>\n",
       "      <td>Donald G. McNeil Jr. and Pam Belluck</td>\n",
       "      <td>Orders for abortion pills by women in seven La...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>Nukes and the UN: a Historic Treaty to Ban Nuc...</td>\n",
       "      <td>Ira Helfand</td>\n",
       "      <td>Email \\nIn an historic move the United Nations...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>EXCLUSIVE: Islamic State Supporters Vow to ‘Sh...</td>\n",
       "      <td>Aaron Klein and Ali Waked</td>\n",
       "      <td>JERUSALEM  —   Islamic State sympathizers and ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>Humiliated Hillary Tries To Hide What Camera C...</td>\n",
       "      <td>Amanda Shea</td>\n",
       "      <td>Humiliated Hillary Tries To Hide What Camera C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>Andrea Tantaros of Fox News Claims Retaliation...</td>\n",
       "      <td>Jim Dwyer</td>\n",
       "      <td>Andrea Tantaros, a former Fox News host, charg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>How Hillary Clinton Became a Hawk - The New Yo...</td>\n",
       "      <td>Mark Landler</td>\n",
       "      <td>Hillary Clinton sat in the hideaway study off ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20770</th>\n",
       "      <td>20770</td>\n",
       "      <td>HUMA ABEDIN SWORE UNDER OATH SHE GAVE UP ‘ALL ...</td>\n",
       "      <td>Iron Sheik</td>\n",
       "      <td>Home › POLITICS | US NEWS › HUMA ABEDIN SWORE ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20771</th>\n",
       "      <td>20771</td>\n",
       "      <td></td>\n",
       "      <td>Letsbereal</td>\n",
       "      <td>DYN's Statement on Last Week's Botnet Attack h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20772</th>\n",
       "      <td>20772</td>\n",
       "      <td></td>\n",
       "      <td>beersession</td>\n",
       "      <td>Kinda reminds me of when Carter gave away the ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20773</th>\n",
       "      <td>20773</td>\n",
       "      <td>Australia to hunt down anti-vax nurses and pro...</td>\n",
       "      <td>Vicki Batts</td>\n",
       "      <td>Australia to hunt down anti-vax nurses and pro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20774</th>\n",
       "      <td>20774</td>\n",
       "      <td>Government Report: Islamists Building ’Paralle...</td>\n",
       "      <td>Liam Deacon</td>\n",
       "      <td>Aided by a politically correct culture of “tol...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20775</th>\n",
       "      <td>20775</td>\n",
       "      <td>How this WWII airman is helping veterans heal ...</td>\n",
       "      <td>Arnaldo Rodgers</td>\n",
       "      <td>‹ › Arnaldo Rodgers is a trained and educated ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20776</th>\n",
       "      <td>20776</td>\n",
       "      <td>Trump Campaign Says Hillary Supporter Tried As...</td>\n",
       "      <td>Jameson Parker</td>\n",
       "      <td>Donald Trump was rushed from a rally stage by ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20777</th>\n",
       "      <td>20777</td>\n",
       "      <td>Editor of Austria’s Largest Paper Charged with...</td>\n",
       "      <td>admin</td>\n",
       "      <td>Breitbart October 26, 2016 \\nAn editor of Aust...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20778</th>\n",
       "      <td>20778</td>\n",
       "      <td>This Is a Jobs Report That Democrats Can Boast...</td>\n",
       "      <td>Neil Irwin</td>\n",
       "      <td>There’s not much to say about the July jobs nu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20779</th>\n",
       "      <td>20779</td>\n",
       "      <td>Christians in 2017 ’Most Persecuted Group in t...</td>\n",
       "      <td>Thomas D. Williams, Ph.D.</td>\n",
       "      <td>In many parts of the world, Christians gatheri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20780</th>\n",
       "      <td>20780</td>\n",
       "      <td>Florida Woman Charged in Death of Infant in ‘C...</td>\n",
       "      <td>Christine Hauser</td>\n",
       "      <td>Early on Oct. 6, Erin   was awakened by the so...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20781</th>\n",
       "      <td>20781</td>\n",
       "      <td>Time is Running Out to Stop Kratom Ban – Need ...</td>\n",
       "      <td>Heather Callaghan</td>\n",
       "      <td>By Brandon Turbeville When the DEA announced t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20782</th>\n",
       "      <td>20782</td>\n",
       "      <td>The Fix Is In: NBC Affiliate Accidentally Post...</td>\n",
       "      <td>The Doc</td>\n",
       "      <td>Home » Headlines » World News » The Fix Is In:...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20783</th>\n",
       "      <td>20783</td>\n",
       "      <td>Samsung, Kim Jong-un, Rex Tillerson: Your Morn...</td>\n",
       "      <td>Charles McDermid</td>\n",
       "      <td>Good morning.  Here’s what you need to know: •...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20784</th>\n",
       "      <td>20784</td>\n",
       "      <td>Comment on World Heaves Sigh of Relief after T...</td>\n",
       "      <td>Debbie Menon</td>\n",
       "      <td>Finian Cunningham has written extensively on...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20785</th>\n",
       "      <td>20785</td>\n",
       "      <td>Ann Coulter: How to Provide Universal Health C...</td>\n",
       "      <td>Ann Coulter</td>\n",
       "      <td>The first sentence of Congress’ Obamacare repe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20786</th>\n",
       "      <td>20786</td>\n",
       "      <td>Government Forces Advancing at Damascus-Aleppo...</td>\n",
       "      <td>nan</td>\n",
       "      <td>#FROMTHEFRONT #MAPS 22.11.2016 - 1,361 views 5...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20787</th>\n",
       "      <td>20787</td>\n",
       "      <td>Sally Yates Won’t Say If Trump Was Wiretapped ...</td>\n",
       "      <td>Ian Mason</td>\n",
       "      <td>Former Deputy Attorney General Sally Yates dec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20788</th>\n",
       "      <td>20788</td>\n",
       "      <td>Maine’s Gov. LePage Threatens To ‘Investigate’...</td>\n",
       "      <td>Joe Clark</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20789</th>\n",
       "      <td>20789</td>\n",
       "      <td>Sen. McConnell: The Supreme Court Vacancy Was ...</td>\n",
       "      <td>Warner Todd Huston</td>\n",
       "      <td>Senate Majority Leader Mitch McConnell (R, KY)...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20790</th>\n",
       "      <td>20790</td>\n",
       "      <td>Nikki Haley Blasts U.N. Human Rights Office fo...</td>\n",
       "      <td>Adam Shaw</td>\n",
       "      <td>U. S Ambassador to the United Nations Nikki Ha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20791</th>\n",
       "      <td>20791</td>\n",
       "      <td>Lawyer Who Kept Hillary Campaign Chief Out of ...</td>\n",
       "      <td>Daniel Greenfield</td>\n",
       "      <td>Lawyer Who Kept Hillary Campaign Chief Out of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20792</th>\n",
       "      <td>20792</td>\n",
       "      <td>Jakarta Bombing Kills Three Police Officers, L...</td>\n",
       "      <td>John Hayward</td>\n",
       "      <td>Two suicide bombers attacked a bus station in ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20793</th>\n",
       "      <td>20793</td>\n",
       "      <td>Idiot Who Destroyed Trump Hollywood Star Gets ...</td>\n",
       "      <td>Robert Rich</td>\n",
       "      <td>Share This \\nAlthough the vandal who thought i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20794</th>\n",
       "      <td>20794</td>\n",
       "      <td>Trump: Putin ’Very Smart’ to Not Retaliate ove...</td>\n",
       "      <td>Lee Stranahan</td>\n",
       "      <td>Donald Trump took to Twitter Friday to praise ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20795</th>\n",
       "      <td>20795</td>\n",
       "      <td>Rapper T.I.: Trump a ’Poster Child For White S...</td>\n",
       "      <td>Jerome Hudson</td>\n",
       "      <td>Rapper T. I. unloaded on black celebrities who...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20796</th>\n",
       "      <td>20796</td>\n",
       "      <td>N.F.L. Playoffs: Schedule, Matchups and Odds -...</td>\n",
       "      <td>Benjamin Hoffman</td>\n",
       "      <td>When the Green Bay Packers lost to the Washing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20797</th>\n",
       "      <td>20797</td>\n",
       "      <td>Macy’s Is Said to Receive Takeover Approach by...</td>\n",
       "      <td>Michael J. de la Merced and Rachel Abrams</td>\n",
       "      <td>The Macy’s of today grew from the union of sev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20798</th>\n",
       "      <td>20798</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>Alex Ansary</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20799</th>\n",
       "      <td>20799</td>\n",
       "      <td>What Keeps the F-35 Alive</td>\n",
       "      <td>David Swanson</td>\n",
       "      <td>David Swanson is an author, activist, journa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20800 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0          0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1          1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
       "2          2                  Why the Truth Might Get You Fired   \n",
       "3          3  15 Civilians Killed In Single US Airstrike Hav...   \n",
       "4          4  Iranian woman jailed for fictional unpublished...   \n",
       "5          5  Jackie Mason: Hollywood Would Love Trump if He...   \n",
       "6          6  Life: Life Of Luxury: Elton John’s 6 Favorite ...   \n",
       "7          7  Benoît Hamon Wins French Socialist Party’s Pre...   \n",
       "8          8  Excerpts From a Draft Script for Donald Trump’...   \n",
       "9          9  A Back-Channel Plan for Ukraine and Russia, Co...   \n",
       "10        10  Obama’s Organizing for Action Partners with So...   \n",
       "11        11  BBC Comedy Sketch \"Real Housewives of ISIS\" Ca...   \n",
       "12        12  Russian Researchers Discover Secret Nazi Milit...   \n",
       "13        13  US Officials See No Link Between Trump and Russia   \n",
       "14        14  Re: Yes, There Are Paid Government Trolls On S...   \n",
       "15        15  In Major League Soccer, Argentines Find a Home...   \n",
       "16        16  Wells Fargo Chief Abruptly Steps Down - The Ne...   \n",
       "17        17  Anonymous Donor Pays $2.5 Million To Release E...   \n",
       "18        18                          FBI Closes In On Hillary!   \n",
       "19        19  Chuck Todd: ’BuzzFeed Did Donald Trump a Polit...   \n",
       "20        20  News: Hope For The GOP: A Nude Paul Ryan Has J...   \n",
       "21        21  Monica Lewinsky, Clinton Sex Scandal Set for ’...   \n",
       "22        22  Rob Reiner: Trump Is ’Mentally Unstable’ - Bre...   \n",
       "23        23  Massachusetts Cop’s Wife Busted for Pinning Fa...   \n",
       "24        24  Abortion Pill Orders Rise in 7 Latin American ...   \n",
       "25        25  Nukes and the UN: a Historic Treaty to Ban Nuc...   \n",
       "26        26  EXCLUSIVE: Islamic State Supporters Vow to ‘Sh...   \n",
       "27        27  Humiliated Hillary Tries To Hide What Camera C...   \n",
       "28        28  Andrea Tantaros of Fox News Claims Retaliation...   \n",
       "29        29  How Hillary Clinton Became a Hawk - The New Yo...   \n",
       "...      ...                                                ...   \n",
       "20770  20770  HUMA ABEDIN SWORE UNDER OATH SHE GAVE UP ‘ALL ...   \n",
       "20771  20771                                                      \n",
       "20772  20772                                                      \n",
       "20773  20773  Australia to hunt down anti-vax nurses and pro...   \n",
       "20774  20774  Government Report: Islamists Building ’Paralle...   \n",
       "20775  20775  How this WWII airman is helping veterans heal ...   \n",
       "20776  20776  Trump Campaign Says Hillary Supporter Tried As...   \n",
       "20777  20777  Editor of Austria’s Largest Paper Charged with...   \n",
       "20778  20778  This Is a Jobs Report That Democrats Can Boast...   \n",
       "20779  20779  Christians in 2017 ’Most Persecuted Group in t...   \n",
       "20780  20780  Florida Woman Charged in Death of Infant in ‘C...   \n",
       "20781  20781  Time is Running Out to Stop Kratom Ban – Need ...   \n",
       "20782  20782  The Fix Is In: NBC Affiliate Accidentally Post...   \n",
       "20783  20783  Samsung, Kim Jong-un, Rex Tillerson: Your Morn...   \n",
       "20784  20784  Comment on World Heaves Sigh of Relief after T...   \n",
       "20785  20785  Ann Coulter: How to Provide Universal Health C...   \n",
       "20786  20786  Government Forces Advancing at Damascus-Aleppo...   \n",
       "20787  20787  Sally Yates Won’t Say If Trump Was Wiretapped ...   \n",
       "20788  20788  Maine’s Gov. LePage Threatens To ‘Investigate’...   \n",
       "20789  20789  Sen. McConnell: The Supreme Court Vacancy Was ...   \n",
       "20790  20790  Nikki Haley Blasts U.N. Human Rights Office fo...   \n",
       "20791  20791  Lawyer Who Kept Hillary Campaign Chief Out of ...   \n",
       "20792  20792  Jakarta Bombing Kills Three Police Officers, L...   \n",
       "20793  20793  Idiot Who Destroyed Trump Hollywood Star Gets ...   \n",
       "20794  20794  Trump: Putin ’Very Smart’ to Not Retaliate ove...   \n",
       "20795  20795  Rapper T.I.: Trump a ’Poster Child For White S...   \n",
       "20796  20796  N.F.L. Playoffs: Schedule, Matchups and Odds -...   \n",
       "20797  20797  Macy’s Is Said to Receive Takeover Approach by...   \n",
       "20798  20798  NATO, Russia To Hold Parallel Exercises In Bal...   \n",
       "20799  20799                          What Keeps the F-35 Alive   \n",
       "\n",
       "                                          author  \\\n",
       "0                                  Darrell Lucus   \n",
       "1                                Daniel J. Flynn   \n",
       "2                             Consortiumnews.com   \n",
       "3                                Jessica Purkiss   \n",
       "4                                 Howard Portnoy   \n",
       "5                                Daniel Nussbaum   \n",
       "6                                            nan   \n",
       "7                                Alissa J. Rubin   \n",
       "8                                            nan   \n",
       "9                   Megan Twohey and Scott Shane   \n",
       "10                                   Aaron Klein   \n",
       "11                               Chris Tomlinson   \n",
       "12                                 Amando Flavio   \n",
       "13                                    Jason Ditz   \n",
       "14                                  AnotherAnnie   \n",
       "15                                 Jack Williams   \n",
       "16              Michael Corkery and Stacy Cowley   \n",
       "17                                      Starkman   \n",
       "18                                       The Doc   \n",
       "19                                     Jeff Poor   \n",
       "20                                           nan   \n",
       "21                                 Jerome Hudson   \n",
       "22                                       Pam Key   \n",
       "23                                           nan   \n",
       "24          Donald G. McNeil Jr. and Pam Belluck   \n",
       "25                                   Ira Helfand   \n",
       "26                     Aaron Klein and Ali Waked   \n",
       "27                                   Amanda Shea   \n",
       "28                                     Jim Dwyer   \n",
       "29                                  Mark Landler   \n",
       "...                                          ...   \n",
       "20770                                 Iron Sheik   \n",
       "20771                                 Letsbereal   \n",
       "20772                                beersession   \n",
       "20773                                Vicki Batts   \n",
       "20774                                Liam Deacon   \n",
       "20775                            Arnaldo Rodgers   \n",
       "20776                             Jameson Parker   \n",
       "20777                                      admin   \n",
       "20778                                 Neil Irwin   \n",
       "20779                  Thomas D. Williams, Ph.D.   \n",
       "20780                           Christine Hauser   \n",
       "20781                          Heather Callaghan   \n",
       "20782                                    The Doc   \n",
       "20783                           Charles McDermid   \n",
       "20784                               Debbie Menon   \n",
       "20785                                Ann Coulter   \n",
       "20786                                        nan   \n",
       "20787                                  Ian Mason   \n",
       "20788                                  Joe Clark   \n",
       "20789                         Warner Todd Huston   \n",
       "20790                                  Adam Shaw   \n",
       "20791                          Daniel Greenfield   \n",
       "20792                               John Hayward   \n",
       "20793                                Robert Rich   \n",
       "20794                              Lee Stranahan   \n",
       "20795                              Jerome Hudson   \n",
       "20796                           Benjamin Hoffman   \n",
       "20797  Michael J. de la Merced and Rachel Abrams   \n",
       "20798                                Alex Ansary   \n",
       "20799                              David Swanson   \n",
       "\n",
       "                                                    text  label  \n",
       "0      House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1      Ever get the feeling your life circles the rou...      0  \n",
       "2      Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3      Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4      Print \\nAn Iranian woman has been sentenced to...      1  \n",
       "5      In these trying times, Jackie Mason is the Voi...      0  \n",
       "6      Ever wonder how Britain’s most iconic pop pian...      1  \n",
       "7      PARIS  —   France chose an idealistic, traditi...      0  \n",
       "8      Donald J. Trump is scheduled to make a highly ...      0  \n",
       "9      A week before Michael T. Flynn resigned as nat...      0  \n",
       "10     Organizing for Action, the activist group that...      0  \n",
       "11     The BBC produced spoof on the “Real Housewives...      0  \n",
       "12     The mystery surrounding The Third Reich and Na...      1  \n",
       "13     Clinton Campaign Demands FBI Affirm Trump's Ru...      1  \n",
       "14     Yes, There Are Paid Government Trolls On Socia...      1  \n",
       "15     Guillermo Barros Schelotto was not the first A...      0  \n",
       "16     The scandal engulfing Wells Fargo toppled its ...      0  \n",
       "17     A Caddo Nation tribal leader has just been fre...      1  \n",
       "18     FBI Closes In On Hillary! Posted on Home » Hea...      1  \n",
       "19     Wednesday after   Donald Trump’s press confere...      0  \n",
       "20     Email \\nSince Donald Trump entered the electio...      1  \n",
       "21     Screenwriter Ryan Murphy, who has produced the...      0  \n",
       "22     Sunday on MSNBC’s “AM Joy,” actor and director...      0  \n",
       "23     Massachusetts Cop’s Wife Busted for Pinning Fa...      1  \n",
       "24     Orders for abortion pills by women in seven La...      0  \n",
       "25     Email \\nIn an historic move the United Nations...      1  \n",
       "26     JERUSALEM  —   Islamic State sympathizers and ...      0  \n",
       "27     Humiliated Hillary Tries To Hide What Camera C...      1  \n",
       "28     Andrea Tantaros, a former Fox News host, charg...      0  \n",
       "29     Hillary Clinton sat in the hideaway study off ...      0  \n",
       "...                                                  ...    ...  \n",
       "20770  Home › POLITICS | US NEWS › HUMA ABEDIN SWORE ...      1  \n",
       "20771  DYN's Statement on Last Week's Botnet Attack h...      1  \n",
       "20772  Kinda reminds me of when Carter gave away the ...      1  \n",
       "20773  Australia to hunt down anti-vax nurses and pro...      1  \n",
       "20774  Aided by a politically correct culture of “tol...      0  \n",
       "20775  ‹ › Arnaldo Rodgers is a trained and educated ...      1  \n",
       "20776  Donald Trump was rushed from a rally stage by ...      1  \n",
       "20777  Breitbart October 26, 2016 \\nAn editor of Aust...      1  \n",
       "20778  There’s not much to say about the July jobs nu...      0  \n",
       "20779  In many parts of the world, Christians gatheri...      0  \n",
       "20780  Early on Oct. 6, Erin   was awakened by the so...      0  \n",
       "20781  By Brandon Turbeville When the DEA announced t...      1  \n",
       "20782  Home » Headlines » World News » The Fix Is In:...      1  \n",
       "20783  Good morning.  Here’s what you need to know: •...      0  \n",
       "20784    Finian Cunningham has written extensively on...      1  \n",
       "20785  The first sentence of Congress’ Obamacare repe...      0  \n",
       "20786  #FROMTHEFRONT #MAPS 22.11.2016 - 1,361 views 5...      1  \n",
       "20787  Former Deputy Attorney General Sally Yates dec...      0  \n",
       "20788  Google Pinterest Digg Linkedin Reddit Stumbleu...      1  \n",
       "20789  Senate Majority Leader Mitch McConnell (R, KY)...      0  \n",
       "20790  U. S Ambassador to the United Nations Nikki Ha...      0  \n",
       "20791  Lawyer Who Kept Hillary Campaign Chief Out of ...      1  \n",
       "20792  Two suicide bombers attacked a bus station in ...      0  \n",
       "20793  Share This \\nAlthough the vandal who thought i...      1  \n",
       "20794  Donald Trump took to Twitter Friday to praise ...      0  \n",
       "20795  Rapper T. I. unloaded on black celebrities who...      0  \n",
       "20796  When the Green Bay Packers lost to the Washing...      0  \n",
       "20797  The Macy’s of today grew from the union of sev...      0  \n",
       "20798  NATO, Russia To Hold Parallel Exercises In Bal...      1  \n",
       "20799    David Swanson is an author, activist, journa...      1  \n",
       "\n",
       "[20800 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_fpath = 'train.csv'\n",
    "train_raw_data = pd.read_csv(train_fpath, encoding='utf-8', header=0, keep_default_na=False)\n",
    "train_raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data pre-processing\n",
    "\n",
    "The first step when building a neural network model is getting your data into the proper form to feed into the network. Since we're using embedding layers, we'll need to encode each word with an integer. We'll also want to clean it up a bit.\n",
    "\n",
    "You can see an example of the articles data above. Here are the processing steps, we'll want to take:\n",
    ">* We'll want to get rid of periods and extraneous punctuation.\n",
    "* Also, you might notice that the some texts are delimited with newline characters `\\n`. To deal with those, I'm going to split the text into each text using `\\n` as the delimiter. \n",
    "* Then I can combined all the text back together into one big string.\n",
    "\n",
    "First, let's remove all punctuation. Then get all the text without the newlines and split it into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "texts = [''.join([c for c in text.lower() if c not in punctuation]) for text in train_raw_data['text']]\n",
    "\n",
    "# split by new lines and spaces\n",
    "all_text = ' '.join(texts)\n",
    "\n",
    "# create a list of words\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the words\n",
    "\n",
    "The embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our texts into a list of integers so they can be passed into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Build a dictionary that maps words to integers\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "# use the dict to tokenize each article in text split\n",
    "# store the tokenized texts in text_ints\n",
    "text_ints = []\n",
    "for text in texts:\n",
    "    text_ints.append([vocab_to_int[word] for word in text.split()])\n",
    "\n",
    "# get the labels (0 and 1) from the data set\n",
    "encoded_labels = [label for label in train_raw_data['label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  267449\n",
      "Tokenized text: \n",
      " [[127, 5764, 2270, 41, 359, 84, 142, 2658, 771, 316, 3069, 7497, 2880, 13, 17, 12989, 27687, 10, 361, 662, 152, 3381, 3069, 7497, 10, 1, 12245, 6, 97, 15549, 4152, 1086, 4627, 738, 178164, 955, 169, 5, 2664, 250096, 2670, 11, 12308, 2, 6096, 46120, 60, 8, 66, 1456, 31, 1, 1539, 425, 6, 1, 107, 8, 26, 245165, 304, 728, 598, 29, 154, 2, 5, 127, 269, 2270, 13, 1328, 62, 41, 64, 128, 31, 1, 243698, 425, 8, 14, 162, 13, 2088, 58, 7, 56, 598, 599, 22, 63949, 771, 4052, 7, 1, 295, 12, 626, 73, 364, 7, 115, 21, 1039, 2, 114, 385, 249, 1402, 1, 5899, 332, 10, 1, 3192, 5344, 359, 1141, 40, 13, 24, 598, 30, 206, 58, 1084, 5, 2260, 24, 48, 3, 1, 194, 478, 13643, 14, 41, 81, 128, 598, 7282, 1, 194, 13643, 4, 269, 5899, 272, 3, 1, 127, 443, 4067, 4, 4203, 5344, 7, 22, 582, 12, 5648, 364, 13, 36, 493, 1442, 6, 266, 2, 142, 54, 30, 3284, 1617, 261, 23, 188, 63, 26, 771, 369, 58, 4203, 478, 858, 3069, 7497, 325, 1, 140, 107, 20529, 11, 26, 2260, 295, 23037, 77, 2668, 131, 1, 295, 27, 1273, 3, 1, 2376, 3, 364, 7, 1146, 2, 21, 8717, 2, 1, 311, 213, 5072, 35, 3069, 7497, 39683, 361, 1189, 152, 3, 388, 41, 81, 128, 7, 26, 12, 23, 1, 213, 598, 12, 392, 297, 7, 13, 12, 5648, 1, 364, 6, 843, 3, 3101, 7284, 237451, 41, 81, 128, 2, 21, 2066, 5018, 8765, 11, 5, 4704, 29, 1396, 156, 263, 247, 14, 1735, 359, 437, 2, 7497, 1, 4152, 194, 36, 260, 3433, 2, 10870, 5, 15386, 3, 1990, 54, 114, 167528, 270, 93, 15418, 1108, 4, 1764, 28, 696, 44229, 1108, 3, 74, 1396, 7497, 467, 1, 295, 12, 260, 384, 22, 163, 9, 139894, 6, 5, 2260, 7, 4177, 17286, 1, 576, 108, 11274, 2486, 3021, 13, 12, 5, 37517, 29, 154, 2, 5, 577, 127, 269, 2270, 44395, 7, 771, 115, 20, 46, 1, 270, 3, 106177, 10054, 7, 2270, 147, 23039, 7, 22, 3365, 4, 69, 332, 359, 84, 128, 40, 2658, 771, 19, 1, 194880, 91, 206, 58, 56, 30, 5105, 210, 27204, 5899, 272, 10, 1, 3192, 5344, 359, 1495, 2658, 771, 316, 63, 1, 194, 13643, 6, 279, 1, 269, 5899, 272, 84578, 1495, 13, 316, 63, 1, 858, 3, 1, 4203, 4, 98, 1944, 478, 3069, 7497, 2880, 13, 58, 4, 125, 13, 14027, 59, 1540, 142, 54, 1209, 301, 26, 173, 1, 295, 304, 2166, 7497, 4, 69, 1866, 478, 13643, 40, 5, 355, 1020, 6, 5, 1953, 2167, 4325, 311, 4, 1661, 7497, 1286, 22, 69, 1977, 36, 1, 4627, 2, 533, 37, 269, 6081, 128, 40, 13, 413, 154, 2, 26, 2270, 15, 125, 74, 313, 58, 40, 13, 10, 210, 60, 27, 260, 46, 570, 10, 592, 22905, 7, 598, 444, 1061, 2131, 2384, 3, 26, 771, 2, 7497, 4, 69, 354, 1088, 74, 79, 2, 628, 10, 1, 5822, 1846, 7, 115, 123, 9, 186, 2307, 29, 60, 8, 403, 59, 253, 7, 84, 2047, 26, 8, 1, 213, 63, 52, 60, 8, 403, 59, 253, 7, 2047, 7, 598, 12, 465, 69, 68, 12881, 9964, 4, 56008, 53, 13, 217, 1893, 299, 8, 7, 7497, 8, 1803, 6, 5, 124, 7, 597, 3516, 9384, 4, 12989, 18594, 276, 62, 3671, 3, 1649, 4, 32906, 15, 359, 84, 20, 1, 11846, 2, 8751, 5899, 610, 18761, 18914, 40, 239, 26, 4325, 54, 7, 453, 24173, 10, 1802, 1858, 3, 6939, 32, 136, 128, 53, 217, 2793, 99, 23, 358, 7, 7497, 42, 20, 2, 1080, 9, 26, 15, 4942, 6, 5, 14360, 194, 1007, 16258, 6, 47184, 4, 110462, 13, 27, 5, 3582, 3438, 623, 4629, 3, 185565, 4, 722, 4529, 3047, 5, 10647, 7705, 165, 3, 1, 240, 6, 721, 3314, 1, 194, 127, 1093, 27, 366, 57, 501, 195, 2, 106177, 1006, 6177, 16294, 29, 7, 453, 725, 41, 452, 628, 1, 2245, 3825, 10, 80, 63, 52, 15, 8, 5, 14556, 553, 3, 53, 1, 127, 27, 271, 169, 194, 336, 4, 15, 8, 64, 1, 386, 1539, 425, 6, 1, 107, 40, 12989, 27687, 12989, 8, 5, 40008, 3391, 3, 1, 315, 3, 290, 1290, 31, 4975, 444, 5, 1768, 3, 1, 487, 282, 28, 1171, 2, 628, 80, 73, 5, 610, 3, 1, 1105, 173, 6, 600, 91, 4984, 6, 1798, 80, 73, 1, 1105, 312, 1539, 40225, 11582, 1412, 31, 8, 28, 14984, 1004, 22, 2155, 2, 874, 65, 9, 96, 31, 20, 46, 4409, 73, 2981, 91, 1310, 56, 15, 3726, 28, 8311, 25057, 2013, 33, 115, 128, 80, 10, 592, 22905, 14, 1412, 5764, 6, 9626, 378, 80, 10, 210, 39863, 38, 4218, 11, 80, 10, 417, 1730, 146, 2, 1075, 12989, 5, 21645, 38363, 4218]]\n"
     ]
    }
   ],
   "source": [
    "# stats about vocabulary\n",
    "print('Unique words: ', len((vocab_to_int)))\n",
    "\n",
    "# print tokens in first article\n",
    "print('Tokenized text: \\n', text_ints[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length text: 116\n",
      "Maximum text length: 24195\n"
     ]
    }
   ],
   "source": [
    "# outlier article stats\n",
    "text_lens = Counter([len(x) for x in text_ints])\n",
    "print(\"Zero-length text: {}\".format(text_lens[0]))\n",
    "print(\"Maximum text length: {}\".format(max(text_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seem to have some texts with zero length. And, the maximum text length is way too many steps for our RNN. We'll have to remove any super short texts and truncate super long texts. This removes outliers and should allow our model to train more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of texts before removing outliers:  20800\n",
      "Number of texts after removing outliers:  20684\n"
     ]
    }
   ],
   "source": [
    "print('Number of texts before removing outliers: ', len(text_ints))\n",
    "\n",
    "## remove any articles/labels with zero length from the text_ints list.\n",
    "\n",
    "# get indices of any articles with length 0\n",
    "non_zero_idx = [ii for ii, text in enumerate(text_ints) if len(text) != 0]\n",
    "\n",
    "# remove 0-length articles and their labels\n",
    "text_ints = [text_ints[ii] for ii in non_zero_idx]\n",
    "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
    "\n",
    "print('Number of texts after removing outliers: ', len(text_ints))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Padding sequences\n",
    "\n",
    "To deal with both short and very long articles, we'll pad or truncate all our articles to a specific length. For texts shorter than some `seq_length`, we'll pad with 0s. For texts longer than `seq_length`, we can truncate them to the first `seq_length` words. A good `seq_length`, in this case, is 200.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/minhkhang1795/FakeNews_RNN/master/assets/outliers_padding_ex.png\" width=40%>\n",
    "\n",
    "> Define a function that returns an array `features` that contains the padded data, of a standard size, that we'll pass to the network. \n",
    "* The data should come from `text_ints`, since we want to feed integers to the network. \n",
    "* Each row should be `seq_length` elements long. \n",
    "* For articles shorter than `seq_length` words, **left pad** with 0s. That is, if the text is `['best', 'movie', 'ever']`, `[117, 18, 128]` as integers, the row will look like `[0, 0, 0, ..., 0, 117, 18, 128]`. \n",
    "* For articles longer than `seq_length`, use only the first `seq_length` words as the feature vector.\n",
    "\n",
    "As a small example, if the `seq_length=10` and an input article is: \n",
    "```\n",
    "[117, 18, 128]\n",
    "```\n",
    "The resultant, padded sequence should be: \n",
    "\n",
    "```\n",
    "[0, 0, 0, 0, 0, 0, 0, 117, 18, 128]\n",
    "```\n",
    "\n",
    "**The final `features` array should be a 2D array, with as many rows as there are articles, and as many columns as the specified `seq_length`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(text_ints, seq_length):\n",
    "    ''' Return features of text_ints, where each article is padded with 0's \n",
    "        or truncated to the input seq_length.\n",
    "    '''\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(text_ints), seq_length), dtype=int)\n",
    "\n",
    "    # for each article, grab that article and \n",
    "    for i, row in enumerate(text_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   127   5764   2270     41    359     84    142   2658    771    316]\n",
      " [   365    113      1   1722     89    201   5180      1  30320    522]\n",
      " [   202      1    742    209    113     33   1461    361   1851    152]\n",
      " [  1538    686   1271    473      6    785     67   6693     20     46]\n",
      " [     0      0      0      0      0      0      0      0      0      0]\n",
      " [     6    102    379    167  10217   7923      8      1   1256      3]\n",
      " [   365   2234     87   3106     90   8430   3048  15973   1090    141]\n",
      " [  1413     35   1202   2556     28  20017   1408    435      6   6533]\n",
      " [   159    488     39      8   1979      2    123      5   1401   6736]\n",
      " [     5    212    108    738   1638   2534   4536     14    150    193]\n",
      " [  4367      9    601      1   2207    200      7  18466     24   1019]\n",
      " [     1   3512   1842  32299     10      1   9271  52938   1227  21412]\n",
      " [     1   4507   2593      1    782  10922      4   3964   1210      8]\n",
      " [    75    119   2631    295  15145   3334    182   1372     11      1]\n",
      " [  1132     60     18    817     98  11431     10    298    135  10873]\n",
      " [ 27469 243927  52778     12     23      1     94  15583   2022      2]\n",
      " [     1   1644  41599   3027   4546  13046     57    858      4    347]\n",
      " [     5  37793    576   4582    511     27     77     46   7393     63]\n",
      " [   295  13147      6     10    114    659     10    221   3129   3430]\n",
      " [   451     63    159    143    454    798     19     39   2518      6]\n",
      " [   249    151    159     39   2343      1    122     76      5    116]\n",
      " [ 21293    994   6790     31     27   1842      1  18678    673     97]\n",
      " [   573     10   8010  17875  22285   2685      4    304   5940  41661]\n",
      " [  3140  40282    763  12527      9  31753   1365 151440   6887     10]\n",
      " [  2004      9   1875   7383     17    205      6    996   3585     97]\n",
      " [   249      6     28   2527    445      1    101    580     94    478]\n",
      " [  2024     35    462     85  17464      4   2481   3812    100   4392]\n",
      " [ 13026    114   4119      2   3357     53   2528   1491    686  10979]\n",
      " [  9713  19640      5    172    775    105   1040   1301      6      5]\n",
      " [   114     75   2388      6      1 121960    596    175     49  13160]]\n"
     ]
    }
   ],
   "source": [
    "seq_length = 200\n",
    "\n",
    "features = pad_features(text_ints, seq_length=seq_length)\n",
    "\n",
    "# test statements - do not change -\n",
    "assert len(features)==len(text_ints), \"Your features should have as many rows as articles.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "# print first 10 values of the first 30 batches \n",
    "print(features[:30, :10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Test\n",
    "\n",
    "With our data in nice shape, we'll split it into training, validation, and test sets.\n",
    "\n",
    "> Create the training, validation, and test sets from the `train_raw_data`\n",
    "* You'll need to create sets for the features and the labels, `train_x` and `train_y`, for example. \n",
    "* Define a split fraction, `split_frac` as the fraction of data to **keep** in the training set. Usually this is set to 0.8 or 0.9. \n",
    "* Whatever data is left will be split in half to create the validation and *testing* data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(16000, 200) \n",
      "Validation set: \t(2340, 200) \n",
      "Test set: \t\t(2340, 200)\n"
     ]
    }
   ],
   "source": [
    "# split data into training, validation, and test data (features and labels, x and y)\n",
    "split_idx = 16000 # about 80%\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = (len(features)-16000)//2\n",
    "val_x, test_x = remaining_x[:test_idx-2], remaining_x[test_idx-2:-4]\n",
    "val_y, test_y = remaining_y[:test_idx-2], remaining_y[test_idx-2:-4]\n",
    "\n",
    "# print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DataLoaders and Batching\n",
    "\n",
    "After creating training, test, and validation data, we can create **DataLoaders** for this data by following two steps:\n",
    "1. Create a known format for accessing our data, using [TensorDataset](https://pytorch.org/docs/stable/data.html#) which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset.\n",
    "2. Create DataLoaders and batch our training, validation, and test Tensor datasets.\n",
    "\n",
    "```\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "```\n",
    "\n",
    "This is an alternative to creating a generator function for batching our data into full batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 10\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=5)\n",
    "valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size, num_workers=5)\n",
    "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size, num_workers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([10, 200])\n",
      "Sample input: \n",
      " tensor([[ 1971,  1025,    12,  ...,  1808,    54,    33],\n",
      "        [    0,     0,     0,  ...,    10,   210, 10220],\n",
      "        [  208,    35,  9184,  ...,    77,   528,   310],\n",
      "        ...,\n",
      "        [   62,    13,    38,  ...,    79,     4,    89],\n",
      "        [   55, 25986,   623,  ...,   398,    11,   255],\n",
      "        [   63,     5,   212,  ...,   477,     9, 32265]])\n",
      "\n",
      "Sample label size:  torch.Size([10])\n",
      "Sample label: \n",
      " tensor([0, 0, 0, 1, 1, 1, 1, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size())  # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size())  # batch_size\n",
    "print('Sample label: \\n', sample_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Recurrent Neural Network with PyTorch\n",
    "\n",
    "Below is where we'll define the network.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/minhkhang1795/FakeNews_RNN/master/assets/network_diagram.png\" width=40%>\n",
    "\n",
    "The layers are as follows:\n",
    "1. An [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) that converts our word tokens (integers) into embeddings of a specific size.\n",
    "2. An [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm) defined by a hidden_state size and number of layers\n",
    "3. A fully-connected output layer that maps the LSTM layer outputs to a desired output_size\n",
    "4. A sigmoid activation layer which turns all outputs into a value 0-1; return **only the last sigmoid output** as the output of this network.\n",
    "\n",
    "### The Embedding Layer\n",
    "\n",
    "We need to add an [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) because there are 267449+ words in our vocabulary. It is massively inefficient to one-hot encode that many classes. So, instead of one-hot encoding, we can have an embedding layer and use that layer as a lookup table. You could train an embedding layer using Word2Vec, then load it here. But, it's fine to just make a new layer, using it for only dimensionality reduction, and let the network learn the weights.\n",
    "\n",
    "\n",
    "### The LSTM Layer(s)\n",
    "\n",
    "We'll create an [LSTM](https://pytorch.org/docs/stable/nn.html#lstm) to use in our recurrent network, which takes in an input_size, a hidden_dim, a number of layers, a dropout probability (for dropout between multiple layers), and a batch_first parameter.\n",
    "\n",
    "Most of the time, our network will have better performance with more layers; between 2-3. Adding more layers allows the network to learn really complex relationships. \n",
    "\n",
    "Note: `init_hidden` should initialize the hidden and cell state of an lstm layer to all zeros, and move those state to GPU, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if train_on_gpu:\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers,\n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "\n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "\n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]  # get last batch of labels\n",
    "\n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\" Initializes hidden state \"\"\"\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        if train_on_gpu:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the network\n",
    "\n",
    "Here, we'll instantiate the network. First up, defining the hyperparameters.\n",
    "\n",
    "* `vocab_size`: Size of our vocabulary or the range of values for our input, word tokens.\n",
    "* `output_size`: Size of our desired output; the number of class scores we want to output (pos/neg).\n",
    "* `embedding_dim`: Number of columns in the embedding lookup table; size of our embeddings.\n",
    "* `hidden_dim`: Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
    "* `n_layers`: Number of LSTM layers in the network. Typically between 1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(267450, 80)\n",
      "  (lstm): LSTM(80, 256, num_layers=3, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int) + 1 # +1 for the 0 padding + our word tokens\n",
    "output_size = 1\n",
    "embedding_dim = 80\n",
    "hidden_dim = 256\n",
    "n_layers = 3\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "# move model to GPU, if available\n",
    "if train_on_gpu:\n",
    "    net.cuda()\n",
    "    \n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Training\n",
    "\n",
    "Below is the typical training code. We'll also be using a new kind of cross entropy loss, which is designed to work with a single Sigmoid output. [BCELoss](https://pytorch.org/docs/stable/nn.html#bceloss), or **Binary Cross Entropy Loss**, applies cross entropy loss to a single value between 0 and 1.\n",
    "\n",
    "We also have some data and training hyparameters:\n",
    "\n",
    "* `lr`: Learning rate for our optimizer.\n",
    "* `epochs`: Number of times to iterate through the training dataset.\n",
    "* `clip`: The maximum gradient value to clip at (to prevent exploding gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('2checkpoint3_.pth')\n",
    "net.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100... Loss: 0.133824... Val Loss: 0.186468 Accuracy: 0.932479\n",
      "Loss decreased. Saving model...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-45f70ad0bc9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# calculate the loss and perform backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training params\n",
    "epochs = 100\n",
    "clip = 5  # gradient clipping\n",
    "min_loss = np.inf\n",
    "\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    num_correct = 0\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    net.train()\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "\n",
    "        if train_on_gpu:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*inputs.size(0)\n",
    "\n",
    "    # Get validation loss\n",
    "    val_h = net.init_hidden(batch_size)\n",
    "    net.eval()\n",
    "    for inputs, labels in valid_loader:\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "        if train_on_gpu:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        output, val_h = net(inputs, val_h)\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "        # convert output probabilities to predicted class (0 or 1)\n",
    "        pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "        # compare predictions to true label\n",
    "        correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        num_correct += np.sum(correct)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*inputs.size(0)\n",
    "\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    print(\"Epoch: {}/{}...\".format(e + 1, epochs),\n",
    "          \"Loss: {:.6f}...\".format(train_loss),\n",
    "          \"Val Loss: {:.6f}\".format(valid_loss),\n",
    "          \"Accuracy: {:.6f}\".format(num_correct/len(valid_loader.dataset)))\n",
    "    if min_loss >= valid_loss:\n",
    "        torch.save(net.state_dict(), '2checkpoint3_.pth')\n",
    "        min_loss = valid_loss\n",
    "        print(\"Loss decreased. Saving model...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(net.state_dict(), '3checkpointx-.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Testing\n",
    "\n",
    "* **Test data performance:** First, we'll see how our trained model performs on all of our defined test_data, above. We'll calculate the average loss and accuracy over the test data splitted above. The best accuracy this RNN can get is **93.29%**.\n",
    "\n",
    "* **Performance on real test set from Kaggle:** Second, we'll see our RNN performance on the real test set from Kaggle. My submission resulted in **92.09%** for private score and **91.41%** for public score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.186\n",
      "Test accuracy: 93.2479%\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "loader = test_loader\n",
    "# iterate over test data\n",
    "for inputs, labels in loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if train_on_gpu:\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(loader.dataset)\n",
    "print(\"Test accuracy: {:.4f}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on Kaggle test set\n",
    "**test.csv**: A testing training dataset with all the same attributes at **train.csv** without the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fpath = 'test.csv'\n",
    "test_raw_data = pd.read_csv(test_fpath, encoding='utf-8', header=0, keep_default_na=False)\n",
    "\n",
    "# Only keep title and text\n",
    "test_data = []\n",
    "for i in range(len(test_raw_data)):\n",
    "    test_data.append(test_raw_data['title'][i] + ' ' + test_raw_data['text'][i])\n",
    "test_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_article(test_article):\n",
    "    test_article = test_article.lower() # lowercase\n",
    "    # get rid of punctuation\n",
    "    test_text = ''.join([c for c in test_article if c not in punctuation])\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "\n",
    "    # tokens\n",
    "    test_ints = []\n",
    "    test_ints.append([vocab_to_int[word] if word in vocab_to_int else 0 for word in test_words])\n",
    "\n",
    "    return test_ints\n",
    "\n",
    "# test code and generate tokenized article\n",
    "test_ints = tokenize_article(test_data[1])\n",
    "test_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each article, predict whether it's fake news (1) or not (0)\n",
    "def predict(net, test_article, sequence_length=200):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    # tokenize article\n",
    "    test_ints = tokenize_article(test_article)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features(test_ints, seq_length)\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    # get the output from the model\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs in test_data:\n",
    "    pred = predict(net, inputs)\n",
    "    preds.append(int(pred.item()))\n",
    "\n",
    "# -- stats! -- ##\n",
    "print(\"Predictions: \", preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, export the result to submit to the competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('submit.csv', 'w') as f:\n",
    "    f_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    f_writer.writerow(['id', 'label'])\n",
    "    for i in range(len(preds)):\n",
    "        f_writer.writerow([test_raw_data['id'][i], preds[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_data = pd.read_csv('submit.csv', encoding='utf-8', header=0, keep_default_na=False)\n",
    "submit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prin(state_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
